# Level1 directory
Ceci Westbrook, 2024

 # Analysis steps # 
Level 1 analyses were completed using FSL's FEAT. GLMs were performed on the data preprocessed by fmriprep (see the preprocessed/ directory) using 3-column FSL-formatted timing files created from the behavioral data output by the task. (Note that this also requires the motion confound files which were generated in preprocessed/.) Registration and preprocessing was minimized in FEAT and will then be bypassed at level2 using a workaround (see level2/ directory).

Note that analyses were complicated by the fact that not all subjects had all trial types (e.g., some subjects never avoided during approach-avoid conflict). This resulted in empty EV's, which does not cause a problem for the FEATs running, but contrasts involving those EV's will zero out and need to be excluded from further analyses. Thus, this directory also contains code to identify each subject's available runs (pretreatment 1 & 2 and posttreatment 1 and 2) and then which if any conditions were empty for that run.

 # Files in this directory and how to use them #
Data from participants in this study are de-identified but not anonymized. Therefore, for reasons of confidentiality, files including subject numbers, as well as behavioral data, are excluded from this repository, but available upon request from the author.

For analyses, in order of use:
AA_onsets_fromdats.R - this is an R script that reads in the .dat files produced by E-Prime and generates the 3-column FSL timing files. It also creates .tsv files to back-propagate into the BIDS-formatted rawdata directory (move_tsvs_to_BIDS.sh).

find_blank_regressors.R - this file reads in the timing files generated by the prior script and writes out the usable vs. blank regressors by subject (blank_regressors.txt and nonblank_regressors.txt). These files will get used at level2 and the group level to identify contrasts (called 'copes' in FSL parlance) to ignore later. 

make_fsfs_level1.sh - this reads in the template fsf (design_template_level1.fsf) and populates the subject and run numbers using a sed command. Note that this might generate unusable files for subjects missing a run, but they can safely be allowed to crash out. The design template file has to be created by opening up FEAT and clicking through and then saving, unless you're a better coder than I am :)

run_feat_level1.sh (and _arraybatch.sh) - the script that submits the fsf files to the server to run the glms. This is written using SLURM syntax but can be edited to run locally.

Accessory files:
allQC.R - the main R script to assist with QC and data exclusion. Reads in data from various sources (including excel database) to create an omnibus database to use for data examination and exclusion, as well as later in other analyses. (As mentioned above, these files, such as the database, are not included in this repository for confidentiality reasons.)

check_for_behav_data.sh - checks for existence of the dat files by subject and run and creates a machine-readable text file (behav_data.txt) which I used for QC. Some subjects had to be excluded due to missing behavioral data, and this is how I identified them.

copenames.txt - a list of which cope is which contrast, for convenience :)

check_for_level1_errors.R - a helper script to look for copes that didn't run correctly, based on searching for cope 14.

NOT included in this repository:
subjects_postrun1.txt etc. - lists of subjects per run, since not all subjects had all runs, required for step 2. Not included due to containing subject numbers.
